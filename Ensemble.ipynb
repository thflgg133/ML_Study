{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensemble.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1QZ_MaHKJgla9K_bOPgRTrPYvICuWdYUx","authorship_tag":"ABX9TyOapS2L//du2jI07n6+46Yd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AQkfJGIRI58","executionInfo":{"status":"ok","timestamp":1632543650579,"user_tz":-540,"elapsed":935,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"087c91e8-3e84-44d1-b930-0b1b20e3a1c9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"M4HMMCo4Z1ec"},"source":["# 앙상블 모형의 이해\n","어떠한 한 현상에 대한 답을 얻는다고 가정해보자, 많은 경우에 한 명의 전문가보다 여려 명의 일반인들의 의견이 더 나은 경우가 있다.\n","\n","위 예제와 비슷하게, 하나의 좋은 모형(회귀,분류)으로부터 예측을 하는 것보다 여러 개의 모형으로부터 예측을 수집하는 것이 더 좋은 예측을 할 수 있다.\n","이러한 여러 개의 모형을 앙상블이라고 부르고, 여러 개의 모형을 조화롭게 학습시키는 것을 앙상블 학습이라고 한다.\n","그리고 6주차에서 배운 결정 트리 모형이 하나가 아니라, 훈련 세트를 무작위로 다른 서브셋으로 만들어서 결정 트리 분류기를 만들고, 많은 모형들 중에서 가장 많은 선택을 받은 클래스를 예측하는 앙상블 모형을 랜덤포레스트라고 한다.\n","오늘날의 랜덤포레스트 모델은 가장 강력한 머신러닝 알고리즘 하나이다.\n","그리고 머신러닝 대회에서 우승하는 솔루션들은 대부분 앙상블 방법을 사용하여서 최고 성능을 낸다.\n","뒤에서 앙상블 방법들 중 배깅, 부스팅, 스태킹을 설명할 것이다."]},{"cell_type":"markdown","metadata":{"id":"EhZzyVsyZ35I"},"source":["하나의 데이터셋을 여러종류의 분류기들로 훈련시켰다고 가정해보자.\n","위에서 언급한대로 하나의 좋은 모델을 사용하는 것보다, 여러 종류의 분류기들이 가장 많이 예측한 클래스를 예측하는 것이 더 좋은 분류기를 만드는 매우 간단한 방법이다.\n","이렇게 다수결의 투표로 정해지는 분류기를 hard voting(집접 투표) 분류기라고 한다. \n","놀랍게도 위 모델 중 가장 성능이 좋은 모델의 정확도보다 다수결을 통해 예측한 앙상블 모델의 성능이 높은 경우가 많다.\n","이렇게 랜덤 추측보다 조금 더 높은 성능을 내는 weak learner(약한 학습기) 가 충분히 많고 다양하다면 strong learner(강한 학습기)가 될 수 있다.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHCycLW8RJiN","executionInfo":{"status":"ok","timestamp":1632494414908,"user_tz":-540,"elapsed":649,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"8ee7dcb0-5d18-4411-ab8b-e21db9623b13"},"source":["from sklearn.datasets import load_iris\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","# 데이터셋 로드\n","iris = load_iris() \n","x = iris.data[:,2:] # 꽃잎의 길이, 너비\n","y = iris.target\n","\n","x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,\n","                                                      random_state=2021, shuffle=True)\n","\n","# 약한 학습기 구축 \n","log_model = LogisticRegression()\n","rnd_model = RandomForestClassifier()\n","svm_model = SVC()\n","\n","# 앙상블 모델 구축\n","voting_model = VotingClassifier(\n","    estimators = [('lr', log_model),\n","                  ('rf', rnd_model),\n","                  ('svc',svm_model)],\n","                  voting='hard' #직접 투표\n",")\n","\n","# 앙상블 모델 학습\n","voting_model.fit(x_train,y_train)\n","\n","# 모델 비교\n","for model in (log_model,rnd_model,svm_model, voting_model):\n","  model.fit(x_train, y_train)\n","  y_pred = model.predict(x_test)\n","  print(model.__class__.__name__, \":\", accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression : 1.0\n","RandomForestClassifier : 0.9333333333333333\n","SVC : 1.0\n","VotingClassifier : 1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"mLFevDvkRvQV"},"source":["# 배깅과 페이스팅\n","앙상블 모형의 좋은 성능을 내기 위해서는 다양한 종류의 오차를 만들어야 하고, 그러기 위해서는 다양한 알고리즘을 사용해야 한다고 배웠다.\n","다양한 오차를 만들기위한 다른 하나의 방법으로는 훈련 세트의 서브셋을 무작위로 구성하여 모델을 학습시키는 것이 있다. 이를 배깅과 페이스팅이라고 부른다.\n","배깅 : 훈련 세트의 중복을 허용하여 샘플링을 하는 방식 (통계학에서는 \"부트스트래핑\"이라고도 부름)\n","페이스팅 : 훈련 세트의 중복을 허용하지 않고 샘플링 하는 방식\n","배깅은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 페이스팅보다 편향이 조금 더 높다.\n","하지만 배깅은 예측기들의 상관관계를 줄이므로 앙상블의 분산을 감소 시킨다.\n","전반적으로 배깅이 더 나은 모델을 만들지만, 시간과 장비가 좋다면 교차검증으로 배깅과 페이스팅을 둘다 해보면 좋다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_ENN9GkZ5y7","executionInfo":{"status":"ok","timestamp":1632496169755,"user_tz":-540,"elapsed":1231,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"d8f2e9b7-6502-446e-a3d0-01e5b18dff1a"},"source":["# 사이킷런에서 배깅과 페이스팅\n","\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# 모델구축\n","bag_model = BaggingClassifier(\n","    DecisionTreeClassifier(), # 약한 학습기 (결정 트리)\n","    n_estimators=500, # 약한 학습기 결정트리 500개 사용\n","    max_samples=0.05, # 0.0 ~ 1.0 사이의 실수선택 (실수 x 샘플 수) percentage\n","    bootstrap=True, # True: 배깅, False: 페이스팅\n","    n_jobs=-1 # 훈련에 사용할 CPU 코어 수 결정 1 2 3  // -1 -> CPU코어 수 전부 사용\n",")\n","\n","# 모델 학습\n","bag_model.fit(x_train, y_train)\n","\n","# 모델 예측 \n","y_pred = bag_model.predict(x_test)\n","\n","# 모델 평가\n","print(bag_model.__class__.__name__, \":\", accuracy_score(y_test,y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BaggingClassifier : 0.9555555555555556\n"]}]},{"cell_type":"markdown","metadata":{"id":"VQ3DXrAnbAQb"},"source":["# oob 평가\n","배깅(중복 허용 샘플링)을 하다보면 평균적으로 훈련 샘플의 약 63%정도만 추출되고 나머지 약 37%는 추출되지 않고, 이렇게 추출되지 않은 샘플들을 oob(out-of-bag)샘플이라고 부른다.\n","예측기가 훈련되는 동안에는 oob샘플을 사용하지 않으므로, 검증 세트나 교차 검증을 사용하지 않고 oob샘플만을 가지고 모델 최적화를 위한 평가를 할 수 있다.\n","앙상블의 평가는 각 예측기의 oob평가의 평균으로 확인한다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLgFGvpdXU3d","executionInfo":{"status":"ok","timestamp":1632496351119,"user_tz":-540,"elapsed":1365,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"4e1db9f5-4120-4885-e97a-66f8dd236efd"},"source":["# 모델 구축\n","bag_model = BaggingClassifier(\n","    base_estimator = DecisionTreeClassifier(),\n","    n_estimators = 500,\n","    bootstrap = True,\n","    n_jobs = -1,\n","    oob_score = True # oob평가를 위해 True를 지정한다.\n",")\n","\n","# 모델 학습\n","bag_model.fit(x_train,y_train)\n","\n","# 모델 평가(oob_score_)\n","print('oob_score : ',bag_model.oob_score_)\n","\n","# 모델 평가\n","y_pred = bag_model.predict(x_test)\n","print('test_score : ',accuracy_score(y_test,y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["oob_score :  0.9523809523809523\n","test_score :  0.9333333333333333\n"]}]},{"cell_type":"markdown","metadata":{"id":"7NH_F22dXVNm"},"source":["🚩 랜덤 패치와 랜덤 서브스페이스\n","위에서는 훈련 샘플을 랜덤 샘플링하여 각 예측기의 오차 다양성을 주었지만, 이번에는 훈련 데이터들의 입력 특성들을 무작위로 샘플링하여 예측기를 만들어서 예측기에 대한 오차 다양성을 줄 수 있다.\n","훈련 데이터들의 입력 특성들을 무작위로 샘플링하는 것을 랜덤 패치 방식과 랜덤 서브스페이스 방식이라고 한다.\n","\n","랜덤 패치 방식 : 훈련 특성과 샘플을 모두 샘플링하는 방식\n","- ex) (bootstrap=True or False, max_samples<1.0, bootstrap_features=True, max_features<1.0)\n","\n","랜덤 서브스페이스 방식 : 훈련 샘플은 모두 사용하고, 특성만 샘플링 하는 것\n","- ex) (bootstrap=False, max_samples=1.0, bootstrap_features=True, max_features<1.0)"]},{"cell_type":"markdown","metadata":{"id":"th9Q68EFYokv"},"source":["#랜덤 포레스트\n","\n","랜덤포레스트는 일반적으로 배깅방법을 사용한 결정트리 앙상블 모델이다.\n","그래서 BaggingClassifier에 DecisionTreeClassifier를 넣는 대신, RandomForestClassifier를 사용할 수 있다.\n","\n","그래서 RandomForestClassifier는 DecisionTreeClassifier와 BaggingClassifier 매개변수 모두 가지고 있다.\n","\n","랜덤포레스트 모델은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 것이 아니라, 무작위로 선택한 특성들 중에서 최선의 특성을 찾는 방식을 채택하여 무작위성을 더 가지게 된다.\n","\n","이를 통해 약간의 편향은 손해보지만, 더욱 다양한 트리를 만들므로 분산을 전체적으로 낮추어서 더 훌륭한 모델을 만들 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJLsJCZIZs4k","executionInfo":{"status":"ok","timestamp":1632497187702,"user_tz":-540,"elapsed":1411,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"07fc2805-51db-469c-9b6e-15621ece0bec"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# 랜덤포레스트 모델 구축\n","rnd_model = RandomForestClassifier(\n","    n_estimators = 500, # 예측기 500개\n","    max_leaf_nodes = 16, # 자식노드의 최대 개수 \n","    n_jobs = -1 # CPU 코어 구동 개수\n",")\n","\n","# 모델 학습\n","rnd_model.fit(x_train,y_train)\n","\n","# 모델 예측\n","y_pred_rf = rnd_model.predict(x_test)\n","\n","# 모델 평가\n","print(\"rnd_model : \",accuracy_score(y_pred_rf,y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rnd_model :  0.9333333333333333\n"]}]},{"cell_type":"markdown","metadata":{"id":"ycg4pBNxaUWk"},"source":["#엑스트라 트리\n","\n","랜덤포레스트는 앞에서 말한 것 처럼 각 노드에서 무작위로 특성을 뽑은 다음 최적의 특성과 임계값을 선택한다.\n","하지만 엑스트라 트리는 최적의 특성과 임계값을 찾는것 대신, 후보 특성을 사용해 무작위로 분할한 다음에 최상을 분할을 선택한다.\n","이렇게되면 기본적으로 편향이 많은 랜던포레스트보다 더욱 편향이 심해지지만, 분산을 더욱 낮출 수 있게 된다.\n","트리 알고리즘에서는 모든 노드에서 최적의 특성과 임계값을 고르는데 시간이 많이 들지만, 엑스트라 트리를 사용하면 훈련과 예측속도가 빨라진다.\n","엑스트라 트리는 ExtraTreesClassifier를 이용하면 사용할 수 있다.\n","RandomForestClassifier와 ExtraTreesClassifier 중 어떤 것이 더 좋을지는 판단하기 어렵기 때문에, 교차검증을 통해서 서로 비교해보고, 더 나은 모델을 선택하여 그리드 탐색방법을 사용해 하이퍼파라미터 튜닝을 한다."]},{"cell_type":"markdown","metadata":{"id":"Fgo6x4ova4Ms"},"source":["랜덤포레스트는 성능이 좋다는 장점말고, 특성의 상대적 중요도를 측정하기 쉽다.(트리기반 모델은 특성 중요도 제공)\n","사이킷런에서는 어떤 특성을 사용한 노드가 평균적으로 불순도를 감소시키는지 확인하여 특성 중요도를 측정하고, 훈련이 끝나고 난 뒤에 특성마다 자동으로 점수를 계산하고 저장한다.\n","저장된 값은 featureimportances 변수에 저장되어 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0myBLurta4mM","executionInfo":{"status":"ok","timestamp":1632497877216,"user_tz":-540,"elapsed":1827,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"f994533c-80ba-4de1-8732-044bc405e418"},"source":["# 데이터셋 정의\n","x = iris.data[:,:]\n","y = iris.target\n","\n","# 모델 구축\n","rnd_model = RandomForestClassifier(\n","    n_estimators = 500,\n","    n_jobs = -1\n",")\n","\n","# 모델 학습\n","rnd_model.fit(x,y)\n","\n","# 특성 중요도 확인 (전체 특성 중요도 합 : 1)\n","for feature_name,feature_imp in zip(iris['feature_names'],rnd_model.feature_importances_):\n","  print(feature_name,' : ',feature_imp)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sepal length (cm)  :  0.09633497077904665\n","sepal width (cm)  :  0.02150779018491352\n","petal length (cm)  :  0.44425856049200135\n","petal width (cm)  :  0.4378986785440384\n"]}]},{"cell_type":"markdown","metadata":{"id":"mIWR7mxadLZL"},"source":["#🚩 부스팅\n","부스팅이란, 약한 학습기를 여러 개들을 서로 연결하고 보완해가면서 더욱 강한 학습기를 만드는 앙상블 방법이다.\n","다양한 부스팅 방법들이 있지만, 그중에서 가장 인기있는 아다부스트와 그래디언트 부스팅을 소개하겠다."]},{"cell_type":"markdown","metadata":{"id":"KFGPcBJEd6QE"},"source":["1.아다부스트\n","\n","- 아다부스트의 아이디어는 이전 예측기가 과소적합되었던 훈련 샘플의 가중치를 더 높이는 것이다.\n","- 이 덕분에 새로운 예측기는 학습하기 어려운 샘플에 대해 더욱 잘 예측하게 된다.\n","- 예를들어보면,아다부스트에서 첫 번째 예측기를 결정트리로 훈련시키고 예측을 했을 때, 잘못 분류된 훈련 샘플에 대해 가중치를 상대적으로 높이고, 두 번째에는 업데이트된 가중치를 통해서 예측의 오분류를 확인하고 가중치를 높일 것인지 낮출 것인지 결정되면서 반복된다.\n","- 사이킷런에서 제공하는 아다부스트는 다음과 같이 수행해볼 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZN83TNyjd6ck","executionInfo":{"status":"ok","timestamp":1632498336920,"user_tz":-540,"elapsed":649,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"ac6532f0-b072-4f63-b7a4-e8f1b0c4eecf"},"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# 아다부스트 모델 구축\n","# 아다부스트의 학습기 : Decision Tree (max_depth =1) 사용\n","# 학습기 개수(n_estimators) : 200개\n","# SAMME(Stagewise Additive Modeling using a Multiclass Exponential loss function) 알고리즘 사용\n","# 기본 학습기가 확률 추정(predict_proba)이 가능하면 SAMME.R 사용 -> 일반적으로 성능이 더 좋음\n","ada_model = AdaBoostClassifier(\n","    DecisionTreeClassifier(max_depth=1),\n","    n_estimators = 200,\n","    algorithm = 'SAMME.R', # Stagewise Additive Modeling using a Multiclass Exponential loss function\n","    learning_rate=0.5\n",")\n","\n","# 모델 학습\n","ada_model.fit(x,y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AdaBoostClassifier(algorithm='SAMME.R',\n","                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n","                                                         class_weight=None,\n","                                                         criterion='gini',\n","                                                         max_depth=1,\n","                                                         max_features=None,\n","                                                         max_leaf_nodes=None,\n","                                                         min_impurity_decrease=0.0,\n","                                                         min_impurity_split=None,\n","                                                         min_samples_leaf=1,\n","                                                         min_samples_split=2,\n","                                                         min_weight_fraction_leaf=0.0,\n","                                                         presort='deprecated',\n","                                                         random_state=None,\n","                                                         splitter='best'),\n","                   learning_rate=0.5, n_estimators=200, random_state=None)"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"4VBANYfGd6im"},"source":["2.그래디언트 부스팅\n","\n","그래디언트 부스팅은 아다부스팅과 비슷하게 학습 샘플에 대해 오차를 보정하면서 순차적으로 예측기를 추가 한다.\n","\n","하지만 차이점은 아다부스트처럼 각 학습 샘플에 대한 가중치를 업데이트 하는 대신, 이전 예측기가 만든 잔차(residual error)에 새로운 예측기를 학습시키는 것이다.\n","\n","결정트리를 예측기로 활용하여 그래디언트 부스팅의 회귀 문제를 수행해 보겠고, 결정트리와 그래디언트 부스팅을 함께 적용한 이 알고리즘을 보통 그래디언트 부스티드 회귀 트리(Gradient Boosted Regression Tree = GBRT)라고 부른다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"id":"le5qJvucfo_k","executionInfo":{"status":"error","timestamp":1632498923076,"user_tz":-540,"elapsed":254,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"ccb599ed-cd2a-4e17-e845-349f08d8c496"},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","# 결정트리(max_depth=3) 모델 구축 및 학습\n","tree_reg_model_1 = DecisionTreeRegressor(max_depth=3)\n","tree_reg_model_1.fit(x,y)\n","\n","# 첫 번째 학습기에서 발생한 잔차를 목적함수로 모델 학습\n","residual_1 = y - tree_reg_model_1.predict(x)\n","tree_reg_model_2 = DecisionTreeRegressor(max_depth=3)\n","tree_reg_model_2.fit(x,residual_1)\n","\n","# 두 번째 학습기에서 발생한 잔차를 목적함수로 모델 학습\n","residual_2 = y - tree_reg_model_2.predict(x)\n","tree_reg_model_3 = DecisionTreeRegressor(max_depth=3)\n","tree_reg_model_3.fit(x,residual_2)\n","\n","# 새로운 데이터를 세 개의 트리를 포함한 앙상블 모델로 예측\n","x_new = [[1.4,0.2]]\n","prediction = sum(tree.predict(x_new) for tree in [tree_reg_model_1,tree_reg_model_2,tree_reg_model_3])\n","prediction"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-b07966cd261c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree_reg_model_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree_reg_model_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree_reg_model_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-b07966cd261c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree_reg_model_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree_reg_model_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree_reg_model_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \"\"\"\n\u001b[1;32m    418\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    389\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 4 and input n_features is 2 "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JI5L5UB-hBjM","executionInfo":{"status":"ok","timestamp":1632499010740,"user_tz":-540,"elapsed":257,"user":{"displayName":"민기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13676423156231281022"}},"outputId":"32a9a040-3f90-4f4c-a32b-055c2eccafd4"},"source":["# 사이킷런에서 제공하는 GBRT 앙상블을 다음과 같이 간단하게 훈련시킬 수 있다.\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","# GBRT 모형 구축\n","# GBRT 앙상블 모형도 마찬가지로 n_estimators, max_depth, min_samples_leaf 등을 통해 모델 규제가 가능하다.\n","# 추가적으로 learning_rate가 각 트리의 기여 정도를 조절한다.\n","# learning_rate가 0.1보다 낮게 설정되면 훈련을 위한 트리가 더 많이 필요하지만 성능은 좋아진다.\n","# 이러한 방식을 축소(shrinkage)라고 부르는 규제 방법이다.\n","gbrt = GradientBoostingRegressor(max_depth = 3,\n","                                 n_estimators = 3,\n","                                 learning_rate = 1)\n","\n","# GBRT 모형 학습\n","gbrt.fit(x,y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n","                          init=None, learning_rate=1, loss='ls', max_depth=3,\n","                          max_features=None, max_leaf_nodes=None,\n","                          min_impurity_decrease=0.0, min_impurity_split=None,\n","                          min_samples_leaf=1, min_samples_split=2,\n","                          min_weight_fraction_leaf=0.0, n_estimators=3,\n","                          n_iter_no_change=None, presort='deprecated',\n","                          random_state=None, subsample=1.0, tol=0.0001,\n","                          validation_fraction=0.1, verbose=0, warm_start=False)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"9sr9fYPfhgoc"},"source":["# 최적의 estimators수를 찾기 위한 간단한 방법\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# train set과 validation set을 8:2로 분리\n","x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2)\n","\n","# GBRT(max_depth=3,예측기 수=120) 모형 구축\n","gbrt = GradientBoostingRegressor(max_depth=3, n_estimators=120)\n","\n","# GBRT 모형 학습\n","gbrt.fit(x_train,y_train)\n","\n","# staged_predict를 활용하여 훈련 각 단계에서 앙상블에 의해 만들어진 예측을 반복자로 반환\n","errors = [mean_squared_error(y_val,y_pred) for y_pred in gbrt.staged_predict(x_val)]\n","\n","# validation 검증 결과에서 가장 좋은 성능을 보인 예측기 수를 추출 \n","bst_estimators_num = np.argmin(errors)\n","\n","# 최고의 일반화 성능을 가진 하이퍼파라미터(n_estimators)를 가지고 재 모델 구축\n","print(\"best_est_num : \",bst_estimators_num)\n","gbrt_best = GradientBoostingRegressor(max_depth=3,n_estimators=bst_estimators_num)\n","\n","# 데이터셋(train + valid)를 가지고 학습\n","gbrt_best.fit(x,y)\n","\n","\n","--------------------------------------------------------------------\n","조기종료(early stopping)를 활용한 \n","# warm_start : fit 메서드 호출될 때마다 기존 트리 유지 및 훈련 추가할 수 있게 해줌\n","# subsample : 각 트리는 무작위로 선택된 25% 훈련 샘플로 학습 => 편향 상승 => 분산 감소 => 훈련 속도 상승\n","# 위 subsample 방법을 \"확률적 그래디언트 부스팅\"이라 부른다.\n","gbrt = GradientBoostingRegressor(max_depth=3, warm_start=True,subsample=0.25)\n","\n","min_val_error = float('inf')\n","error_going_up = 0\n","best_estimator = 0\n","for n_estimators in range(1,120):\n","  gbrt.n_estimators = n_estimators\n","  gbrt.fit(x_train,y_train)\n","  y_pred = gbrt.predict(x_val)\n","  val_error = mean_squared_error(y_val, y_pred)\n","  if val_error < min_val_error:\n","    min_val_error = val_error\n","    best_estimator = n_estimators\n","    error_going_up = 0\n","  else:\n","    error_going_up += 1 # 성능 향상이 되지 않을 때마다 +1 \n","    if error_going_up == 5:\n","      break # 성능 향상 연속 5회 : 조기 종료"],"execution_count":null,"outputs":[]}]}